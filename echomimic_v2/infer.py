#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®å¤åçš„ EchoMimicV2 æ¨ç†è„šæœ¬
è§£å†³äº†ç»„ä»¶åŠ è½½å’Œ audio_guider çš„é—®é¢˜
"""
import os
import sys
import torch
import argparse
import yaml
from pathlib import Path
from diffusers import AutoencoderKL, DDIMScheduler
from transformers import CLIPVisionModelWithProjection

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.pipelines.pipeline_echomimicv2 import EchoMimicV2Pipeline
from src.models.unet_2d_condition import UNet2DConditionModel
from src.models.unet_3d_emo import EMOUNet3DConditionModel
from src.models.pose_encoder import PoseEncoder
from src.models.whisper.audio2feature import load_audio_model

def parse_args():
    parser = argparse.ArgumentParser(description="EchoMimicV2 æ¨ç†è„šæœ¬")
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="fp16", help="æ•°æ®ç±»å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    return parser.parse_args()

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def load_models_manually(config, device="cuda", dtype=torch.float16):
    """æ‰‹åŠ¨åŠ è½½æ‰€æœ‰ç»„ä»¶ - ä¿®å¤ç‰ˆæœ¬"""
    print("ğŸ”§ å¼€å§‹åŠ è½½æ¨¡å‹ç»„ä»¶...")
    
    # è®¾ç½®æ¨¡å‹è·¯å¾„
    if "BadToBest/EchoMimicV2" in str(config.get('model_path', '')):
        pretrained_model_path = "BadToBest/EchoMimicV2"
    else:
        pretrained_model_path = config.get('model_path', './pretrained_weights')
    
    components = {}
    
    try:
        # 1. åŠ è½½ VAE
        print("ğŸ“¦ åŠ è½½ VAE...")
        vae = AutoencoderKL.from_pretrained(
            pretrained_model_path, 
            subfolder="vae",
            torch_dtype=dtype
        ).to(device)
        components['vae'] = vae
        print("âœ… VAE åŠ è½½å®Œæˆ")
        
        # 2. åŠ è½½è°ƒåº¦å™¨
        print("ğŸ“¦ åŠ è½½ Scheduler...")
        scheduler = DDIMScheduler.from_pretrained(
            pretrained_model_path, 
            subfolder="scheduler"
        )
        components['scheduler'] = scheduler
        print("âœ… Scheduler åŠ è½½å®Œæˆ")
        
        # 3. åŠ è½½ reference_unet
        print("ğŸ“¦ åŠ è½½ Reference UNet...")
        reference_unet = UNet2DConditionModel.from_pretrained(
            pretrained_model_path,
            subfolder="reference_unet",
            torch_dtype=dtype
        ).to(device)
        components['reference_unet'] = reference_unet
        print("âœ… Reference UNet åŠ è½½å®Œæˆ")
        
        # 4. åŠ è½½ denoising_unet  
        print("ğŸ“¦ åŠ è½½ Denoising UNet...")
        denoising_unet = EMOUNet3DConditionModel.from_pretrained(
            pretrained_model_path,
            subfolder="denoising_unet", 
            torch_dtype=dtype
        ).to(device)
        components['denoising_unet'] = denoising_unet
        print("âœ… Denoising UNet åŠ è½½å®Œæˆ")
        
        # 5. åŠ è½½ pose_encoder
        print("ğŸ“¦ åŠ è½½ Pose Encoder...")
        pose_encoder = PoseEncoder.from_pretrained(
            pretrained_model_path,
            subfolder="pose_encoder",
            torch_dtype=dtype
        ).to(device)
        components['pose_encoder'] = pose_encoder
        print("âœ… Pose Encoder åŠ è½½å®Œæˆ")
        
        # 6. åŠ è½½ audio_guider (Audio2Feature) - å…³é”®ä¿®å¤ï¼
        print("ğŸ“¦ åŠ è½½ Audio Processor...")
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„éŸ³é¢‘æ¨¡å‹è·¯å¾„
        audio_model_paths = [
            "./pretrained_weights_old/audio_processor/tiny.pt",
            "./pretrained_weights/audio_processor/tiny.pt", 
            config.get('audio_model_path', './pretrained_weights/audio_processor/tiny.pt')
        ]
        
        audio_guider = None
        for audio_path in audio_model_paths:
            if os.path.exists(audio_path):
                try:
                    audio_guider = load_audio_model(model_path=audio_path, device=device)
                    print(f"âœ… Audio Processor åŠ è½½å®Œæˆ: {audio_path}")
                    break
                except Exception as e:
                    print(f"âš ï¸ éŸ³é¢‘æ¨¡å‹è·¯å¾„ {audio_path} åŠ è½½å¤±è´¥: {e}")
                    continue
        
        if audio_guider is None:
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„éŸ³é¢‘æ¨¡å‹æ–‡ä»¶ã€‚å°è¯•çš„è·¯å¾„: {audio_model_paths}")
        
        components['audio_guider'] = audio_guider
        
        print("ğŸ‰ æ‰€æœ‰ç»„ä»¶åŠ è½½å®Œæˆï¼")
        return components
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def main():
    args = parse_args()
    
    print(f"ğŸš€ å¯åŠ¨ EchoMimicV2 æ¨ç†...")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {args.config}")
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®è®¾å¤‡å’Œæ•°æ®ç±»å‹
    device = torch.device(args.device)
    dtype = torch.float16 if args.dtype == "fp16" else torch.float32
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    try:
        # æ‰‹åŠ¨åŠ è½½æ‰€æœ‰ç»„ä»¶
        components = load_models_manually(config, device, dtype)
        
        # åˆ›å»ºç®¡é“
        print("ğŸ­ åˆ›å»º EchoMimicV2 ç®¡é“...")
        pipeline = EchoMimicV2Pipeline(**components)
        
        # è·å–æ¨ç†å‚æ•°
        face_img_path = config.get('reference_image')
        audio_path = config.get('audio_path')
        pose_dir = config.get('pose_dir')
        output_dir = config.get('output_dir', './outputs')
        
        print(f"ğŸ–¼ï¸  å‚è€ƒå›¾åƒ: {face_img_path}")
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        print(f"ğŸ¤– å§¿æ€ç›®å½•: {pose_dir}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # è¿è¡Œæ¨ç†
        print("âš¡ å¼€å§‹ç”Ÿæˆæ•°å­—äººè§†é¢‘...")
        result = pipeline(
            face_img_path=face_img_path,
            pose_dir=pose_dir, 
            audio_path=audio_path,
            width=config.get('width', 512),
            height=config.get('height', 512),
            length=config.get('length', None),  # è‡ªåŠ¨æ ¹æ®éŸ³é¢‘é•¿åº¦ç¡®å®š
            guidance_scale=config.get('guidance_scale', 2.0),
            num_inference_steps=config.get('num_inference_steps', 25),
            generator=torch.Generator(device=device).manual_seed(args.seed)
        )
        
        print("âœ… æ¨ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
    except Exception as e:
        print(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()