# -*- coding: utf-8 -*-
# EchoMimic V2 ä¼˜åŒ–ç‰ˆæ¨ç†è„šæœ¬ - RTX 4000 Ada 12GB ä¸“ç”¨
# ä¼˜åŒ–ç›®æ ‡ï¼š5ç§’éŸ³é¢‘2-3åˆ†é’Ÿå®Œæˆï¼Œé¿å…15åˆ†é’Ÿè¶…æ—¶

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
from pathlib import Path
import time
import gc
import psutil
from contextlib import contextmanager

# ==================== GPU å†…å­˜ä¼˜åŒ–é…ç½® ====================
# å¿…é¡»åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—å‰è®¾ç½®
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.6'
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # ç”Ÿäº§ç¯å¢ƒè®¾ä¸º0
os.environ['PYTORCH_CUDA_ALLOC_SYNC'] = '1'  # åŒæ­¥å†…å­˜åˆ†é…

# GPUå†…å­˜åˆ†é…ä¼˜åŒ–
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.75)  # é™ä½åˆ°75%ç»™ç³»ç»Ÿç•™æ›´å¤šç©ºé—´
    torch.cuda.empty_cache()
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.allow_tf32 = True  # å¯ç”¨TF32åŠ é€Ÿ
    #print(f"ğŸš€ GPUä¼˜åŒ–å·²å¯ç”¨: {torch.cuda.get_device_name()}")
    #print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
    print(f"[GPU] ä¼˜åŒ–å·²å¯ç”¨: {torch.cuda.get_device_name()}")
    print(f"[Memory] GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")

# å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(str(Path(__file__).parent))
from src.models.unet import UNet3DConditionModel
#from src.pipelines.pipeline_echomimic import EchoMimicPipeline
from src.pipelines.pipeline_echomimicv2 import EchoMimicV2Pipeline as EchoMimicPipeline
from src.utils.util import save_videos_grid
from src.utils.audio_processor import AudioProcessor

@contextmanager
def gpu_memory_manager():
    """GPUå†…å­˜ç®¡ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    torch.cuda.empty_cache()
    gc.collect()
    yield
    torch.cuda.empty_cache()
    gc.collect()

def print_system_info():
    """æ‰“å°ç³»ç»Ÿèµ„æºä¿¡æ¯"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory/1024**3
        gpu_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_cached = torch.cuda.memory_reserved() / 1024**3
        print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: æ€»è®¡{gpu_memory:.1f}GB, å·²ç”¨{gpu_allocated:.2f}GB, ç¼“å­˜{gpu_cached:.2f}GB")
    
    cpu_percent = psutil.cpu_percent()
    ram = psutil.virtual_memory()
    print(f"ğŸ”§ CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
    print(f"ğŸ§  RAM: {ram.used/1024**3:.1f}GB / {ram.total/1024**3:.1f}GB ({ram.percent:.1f}%)")

def print_gpu_memory(stage=""):
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3
        print(f"ğŸ“Š {stage} - GPUå†…å­˜: å·²ç”¨{allocated:.2f}GB, ç¼“å­˜{cached:.2f}GB, å³°å€¼{max_allocated:.2f}GB")

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def initialize_pipeline(config):
    """åˆå§‹åŒ–ä¼˜åŒ–çš„æ¨ç†ç®¡é“"""
    print("ğŸ”§ åˆå§‹åŒ–æ¨ç†ç®¡é“...")
    print_gpu_memory("ç®¡é“åˆå§‹åŒ–å‰")
    
    # æ¨¡å‹è·¯å¾„
    #model_path = config.get('model_path', './pretrained_weights')
    model_path = config.get('pretrained_base_model_path', './pretrained_weights/sd-image-variations-diffusers')
    
    if not Path(model_path).exists():
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
    
    try:
        with gpu_memory_manager():
            # ä½¿ç”¨ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½å‚æ•°
            pipe = EchoMimicPipeline.from_pretrained(
                model_path,
                torch_dtype=torch.float16,  # åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
                #variant="fp16",
                low_cpu_mem_usage=True,     # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                #use_safetensors=True,
                #device_map="auto"           # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
                device_map="balanced"       # å¹³è¡¡è®¾å¤‡æ˜ å°„
            )
            
            # ç§»åŠ¨åˆ°GPU
            pipe = pipe.to("cuda", torch.float16)
            
            # å¯ç”¨å…³é”®ä¼˜åŒ–
            pipe.enable_vae_slicing()              # VAEåˆ‡ç‰‡å‡å°‘æ˜¾å­˜
            pipe.enable_vae_tiling()               # VAEåˆ†å—å¤„ç†
            pipe.enable_attention_slicing("auto")  # æ³¨æ„åŠ›åˆ‡ç‰‡
            pipe.enable_model_cpu_offload()        # æ¨¡å‹CPUå¸è½½
            
            # å¯ç”¨xFormersä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                pipe.enable_xformers_memory_efficient_attention()
                print("âœ… xFormerså†…å­˜ä¼˜åŒ–å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ xFormersä¸å¯ç”¨: {e}")
            
            # ç¼–è¯‘ä¼˜åŒ–ï¼ˆå¯é€‰ï¼Œé¦–æ¬¡è¿è¡Œä¼šè¾ƒæ…¢ï¼‰
            if config.get('use_torch_compile', False):
                print("ğŸ”¥ å¯ç”¨Torchç¼–è¯‘ä¼˜åŒ–...")
                pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
        
        print("âœ… æ¨ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
        print_gpu_memory("ç®¡é“åˆå§‹åŒ–å")
        
        return pipe
        
    except Exception as e:
        print(f"âŒ ç®¡é“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

def get_audio_duration(audio_path):
    """è·å–éŸ³é¢‘æ—¶é•¿"""
    try:
        import librosa
        audio, sr = librosa.load(audio_path, sr=None)
        duration = len(audio) / sr
        print(f"ğŸµ éŸ³é¢‘ä¿¡æ¯: {duration:.1f}ç§’, {sr}Hz")
        return duration
    except ImportError:
        print("âš ï¸ librosaæœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
        try:
            import wave
            with wave.open(audio_path, 'rb') as wav_file:
                frames = wav_file.getnframes()
                sample_rate = wav_file.getframerate()
                duration = frames / float(sample_rate)
                print(f"ğŸµ éŸ³é¢‘ä¿¡æ¯: {duration:.1f}ç§’, {sample_rate}Hz")
                return duration
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è·å–éŸ³é¢‘é•¿åº¦: {e}")
            return 5.0
    except Exception as e:
        print(f"âš ï¸ éŸ³é¢‘è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ—¶é•¿: {e}")
        return 5.0

def optimize_inference_params(audio_duration, gpu_memory_gb=12):
    """æ ¹æ®éŸ³é¢‘é•¿åº¦å’ŒGPUå†…å­˜ä¼˜åŒ–æ¨ç†å‚æ•°"""
    
    # æ ¹æ®å¯ç”¨GPUå†…å­˜è°ƒæ•´åŸºç¡€å‚æ•°
    memory_factor = min(gpu_memory_gb / 12, 1.0)  # ä»¥12GBä¸ºåŸºå‡†
    
    # RTX 4000 Ada 12GB ä¼˜åŒ–å‚æ•°
    if audio_duration <= 3:
        # è¶…çŸ­éŸ³é¢‘ï¼ˆâ‰¤3ç§’ï¼‰- å¯ä»¥ç”¨æ›´é«˜è´¨é‡
        params = {
            "video_length": int(20 * memory_factor),
            "num_inference_steps": int(25 * memory_factor),
            "guidance_scale": 2.5,
            "height": 512,
            "width": 512,
        }
    elif audio_duration <= 5:
        # çŸ­éŸ³é¢‘ï¼ˆ3-5ç§’ï¼‰- å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
        params = {
            "video_length": int(16 * memory_factor),
            "num_inference_steps": int(20 * memory_factor),
            "guidance_scale": 2.0,
            "height": 512,
            "width": 512,
        }
    elif audio_duration <= 10:
        # ä¸­ç­‰é•¿åº¦ï¼ˆ5-10ç§’ï¼‰
        params = {
            "video_length": int(14 * memory_factor),
            "num_inference_steps": int(18 * memory_factor),
            "guidance_scale": 1.8,
            "height": 512,
            "width": 512,
        }
    else:
        # é•¿éŸ³é¢‘ï¼ˆ>10ç§’ï¼‰ - æ›´æ¿€è¿›ä¼˜åŒ–
        params = {
            "video_length": int(12 * memory_factor),
            "num_inference_steps": int(15 * memory_factor),
            "guidance_scale": 1.5,
            "height": 384,  # ä¸­ç­‰åˆ†è¾¨ç‡
            "width": 384,
        }
    
    # ç¡®ä¿æœ€å°å€¼
    params["video_length"] = max(params["video_length"], 8)
    params["num_inference_steps"] = max(params["num_inference_steps"], 10)
    
    print(f"ğŸ¯ éŸ³é¢‘é•¿åº¦{audio_duration:.1f}s, GPUå†…å­˜{gpu_memory_gb}GB, ä¼˜åŒ–å‚æ•°:")
    for key, value in params.items():
        print(f"   {key}: {value}")
    
    return params

def validate_inputs(config):
    """éªŒè¯è¾“å…¥æ–‡ä»¶"""
    ref_image_path = config.get('reference_image')
    audio_path = config.get('audio_path')
    pose_dir = config.get('pose_dir')
    
    if not all([ref_image_path, audio_path, pose_dir]):
        raise ValueError("âŒ é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦å‚æ•°: reference_image, audio_path, pose_dir")
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    for path, name in [(ref_image_path, "å‚è€ƒå›¾åƒ"), (audio_path, "éŸ³é¢‘æ–‡ä»¶"), (pose_dir, "å§¿åŠ¿ç›®å½•")]:
        if not Path(path).exists():
            raise FileNotFoundError(f"âŒ {name}ä¸å­˜åœ¨: {path}")
    
    return ref_image_path, audio_path, pose_dir

def run_inference(pipe, config):
    """æ‰§è¡Œä¼˜åŒ–çš„æ¨ç†è¿‡ç¨‹"""
    
    # éªŒè¯è¾“å…¥
    ref_image_path, audio_path, pose_dir = validate_inputs(config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(config.get('output_dir', './outputs'))
    output_dir.mkdir(exist_ok=True)
    
    # è·å–éŸ³é¢‘é•¿åº¦
    audio_duration = get_audio_duration(audio_path)
    
    # è·å–GPUå†…å­˜ä¿¡æ¯
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    # ä¼˜åŒ–æ¨ç†å‚æ•°
    infer_params = optimize_inference_params(audio_duration, gpu_memory_gb)
    
    print("ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    print_gpu_memory("æ¨ç†å¼€å§‹å‰")
    
    start_time = time.time()
    max_retries = 2
    
    for attempt in range(max_retries + 1):
        try:
            with gpu_memory_manager():
                # æ‰§è¡Œæ¨ç†
                with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):
                    result = pipe(
                        reference_image=ref_image_path,
                        audio_path=audio_path,
                        pose_dir=pose_dir,
                        video_length=infer_params["video_length"],
                        num_inference_steps=infer_params["num_inference_steps"],
                        guidance_scale=infer_params["guidance_scale"],
                        height=infer_params["height"],
                        width=infer_params["width"],
                        generator=torch.Generator("cuda").manual_seed(config.get('seed', 42)),
                        return_dict=True,
                        callback_on_step_end=lambda step, timestep, latents: print(f"Step {step}/{infer_params['num_inference_steps']}", end='\r') if step % 5 == 0 else None
                    )
            
            break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            
        except torch.cuda.OutOfMemoryError as e:
            print(f"\nâŒ GPUæ˜¾å­˜ä¸è¶³ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
            
            if attempt < max_retries:
                # é™çº§å‚æ•°é‡è¯•
                print("ğŸš¨ ä½¿ç”¨é™çº§å‚æ•°é‡è¯•...")
                scale_factor = 0.7 ** (attempt + 1)  # é€æ¬¡é™çº§
                infer_params = {
                    "video_length": max(int(infer_params["video_length"] * scale_factor), 6),
                    "num_inference_steps": max(int(infer_params["num_inference_steps"] * scale_factor), 8),
                    "guidance_scale": max(infer_params["guidance_scale"] * scale_factor, 1.0),
                    "height": max(int(infer_params["height"] * scale_factor), 256),
                    "width": max(int(infer_params["width"] * scale_factor), 256),
                }
                
                print(f"ğŸ“‰ é™çº§å‚æ•° (ç¼©æ”¾å› å­: {scale_factor:.2f}):")
                for key, value in infer_params.items():
                    print(f"   {key}: {value}")
                
                torch.cuda.empty_cache()
                gc.collect()
                time.sleep(2)  # ç­‰å¾…å†…å­˜é‡Šæ”¾
            else:
                raise
        
        except Exception as e:
            print(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
            if attempt < max_retries:
                print("ğŸ”„ é‡è¯•ä¸­...")
                time.sleep(1)
            else:
                raise
    
    # è®¡ç®—è€—æ—¶
    duration = time.time() - start_time
    print(f"\nâ±ï¸ æ¨ç†å®Œæˆï¼Œè€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
    print_gpu_memory("æ¨ç†å®Œæˆå")
    
    # ä¿å­˜è§†é¢‘
    timestamp = int(time.time())
    output_path = output_dir / f"echomimic_output_{timestamp}.mp4"
    
    if hasattr(result, 'videos') and result.videos is not None:
        save_videos_grid(result.videos, output_path, fps=config.get('output_fps', 25))
        print(f"âœ… è§†é¢‘å·²ä¿å­˜: {output_path}")
        
        # æ€§èƒ½ç»Ÿè®¡
        fps = infer_params["video_length"] / duration
        efficiency = audio_duration / duration
        
        print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   ç”Ÿæˆé€Ÿåº¦: {fps:.2f} å¸§/ç§’")
        print(f"   æ•ˆç‡æ¯”: {efficiency:.3f} (éŸ³é¢‘ç§’æ•°/ç”Ÿæˆç§’æ•°)")
        
        if duration < 180:  # 3åˆ†é’Ÿå†…
            print("ğŸ‰ ä¼˜åŒ–æˆåŠŸï¼ç”Ÿæˆæ—¶é—´åœ¨ç›®æ ‡èŒƒå›´å†…")
        elif duration < 300:  # 5åˆ†é’Ÿå†…
            print("âœ… ç”Ÿæˆå®Œæˆï¼Œæ—¶é—´å¯æ¥å—")
        else:
            print("âš ï¸ ç”Ÿæˆæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–å‚æ•°")
            
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'audio_duration': audio_duration,
            'generation_time': duration,
            'parameters': infer_params,
            'efficiency': efficiency,
            'fps': fps
        }
        
        metadata_path = output_dir / f"metadata_{timestamp}.yaml"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
        print(f"ğŸ“ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
            
    else:
        print("âŒ æ¨ç†ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶")

def main():
    parser = argparse.ArgumentParser(description="EchoMimicä¼˜åŒ–æ¨ç†")
    parser.add_argument("--config", type=str, default="./configs/prompts/infer.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", action="store_true",
                       help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    print("ğŸš€ EchoMimic V2 å¢å¼ºä¼˜åŒ–ç‰ˆæ¨ç†å¯åŠ¨")
    print("ğŸ¯ é’ˆå¯¹RTX 4000 Ada 12GBæ·±åº¦ä¼˜åŒ–")
    print("â±ï¸ ç›®æ ‡ï¼š5ç§’éŸ³é¢‘2-3åˆ†é’Ÿå®Œæˆ")
    print("-" * 50)
    
    if args.verbose:
        print_system_info()
        print("-" * 50)
    
    try:
        # æ£€æŸ¥CUDA
        if not torch.cuda.is_available():
            raise RuntimeError("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥GPUé©±åŠ¨")
        
        # é‡ç½®GPUå†…å­˜ç»Ÿè®¡
        torch.cuda.reset_peak_memory_stats()
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # åˆå§‹åŒ–ç®¡é“
        pipe = initialize_pipeline(config)
        
        # æ‰§è¡Œæ¨ç†
        run_inference(pipe, config)
        
        print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
        
        if args.verbose:
            print("-" * 30)
            print_system_info()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)
    finally:
        # æ¸…ç†èµ„æº
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

if __name__ == "__main__":
    main()